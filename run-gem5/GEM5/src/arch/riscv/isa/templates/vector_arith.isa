output header {{

#define ASSIGN_VD_BIT(idx, bit) \
    ((Vd[(idx)/8] & ~(1 << (idx)%8)) | ((bit) << (idx)%8))

#define COPY_OLD_VD() \
    [[maybe_unused]] RiscvISA::vreg_t old_vd; \
    [[maybe_unused]] decltype(Vd) old_Vd = nullptr; \
    assert(oldDstIdx > -1); \
    xc->getRegOperand(this, oldDstIdx, &old_vd); \
    old_Vd = old_vd.as<std::remove_reference_t<decltype(Vd[0])> >(); \
    memcpy(Vd, old_Vd, VLENB);

#define SET_OLDDST_SRC() \
    oldDstIdx = _numSrcRegs; \
    setSrcRegIdx(_numSrcRegs++, destRegIdxArr[0]);

#define SET_VL_SRC() \
    vlsrcIdx = _numSrcRegs; \
    setSrcRegIdx(_numSrcRegs++, VecRenamedVLReg);

#define SET_VM_SRC() \
    if (!this->vm) { \
        vmsrcIdx = _numSrcRegs; \
        setSrcRegIdx(_numSrcRegs++, RegId(VecRegClass, 0)); \
    }

#define VM_REQUIRED() \
    RiscvISA::vreg_t tmp_v0; \
    uint8_t *v0; \
    if(!this->vm) { \
        assert(vmsrcIdx > 0); \
        xc->getRegOperand(this, vmsrcIdx, &tmp_v0); \
        v0 = tmp_v0.as<uint8_t>(); \
    }

#define VRM_REQUIRED                                                         \
        uint_fast8_t frm = xc->readMiscReg(MISCREG_FRM);                     \
        if (frm > 4)                                                         \
            return std::make_shared<IllegalInstFault>("RM fault", machInst); \
        softfloat_roundingMode = frm;

template<typename Type>
bool inline
carry_out(Type a, Type b, bool carry_in = false) {
    using TypeU = std::make_unsigned_t<Type>;
    TypeU s = *reinterpret_cast<TypeU*>(&a)
            + *reinterpret_cast<TypeU*>(&b) + carry_in;
    return carry_in
        ? (s <= *reinterpret_cast<TypeU*>(&a))
        : (s <  *reinterpret_cast<TypeU*>(&a));
}

template<typename Type>
bool inline
borrow_out(Type a, Type b, bool borrow_in = false) {
    using TypeU = std::make_unsigned_t<Type>;
    return borrow_in
        ? (*reinterpret_cast<TypeU*>(&a) <= *reinterpret_cast<TypeU*>(&b))
        : (*reinterpret_cast<TypeU*>(&a) <  *reinterpret_cast<TypeU*>(&b));
}

}};

def template VectorIntMacroDeclare {{

template<typename ElemType>
class %(class_name)s : public %(base_class)s {
private:
    %(reg_idx_arr_decl)s;
public:
    %(class_name)s(ExtMachInst _machInst);
    using %(base_class)s::generateDisassembly;
};

}};

def template VectorIntMacroConstructor {{

template<typename ElemType>
%(class_name)s<ElemType>::%(class_name)s(ExtMachInst _machInst)
    : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s)
{
    const uint32_t num_microops = vflmul < 1 ? 1 : vflmul;
    StaticInstPtr microop;
    for (int i = 0; i < num_microops; ++i) {
        microop = new %(class_name)sMicro<ElemType>(_machInst, i);
        microop->setDelayedCommit();
        this->microops.push_back(microop);
    }

    this->microops.front()->setFirstMicroop();
    this->microops.back()->setLastMicroop();
}
}};

def template VectorIntMicroDeclare {{

template<typename ElemType>
class %(class_name)s : public %(base_class)s
{
private:
    // vs1, vs2, vs3(old_vd), vm for *.vv, *.vx
    // vs2, (old_vd), vm for *.vi
    RegId srcRegIdxArr[8];
    RegId destRegIdxArr[1];
public:
    %(class_name)s(ExtMachInst _machInst, uint8_t _microIdx);
    Fault execute(ExecContext* xc, Trace::InstRecord* traceData)const override;
    using %(base_class)s::generateDisassembly;
};

}};

def template VectorIntMicroConstructor {{

template<typename ElemType>
%(class_name)s<ElemType>::%(class_name)s(ExtMachInst _machInst, uint8_t _microIdx)
    : %(base_class)s("%(mnemonic)s", _machInst,
                     %(op_class)s, _microIdx)
{
    uint32_t elem_num_per_vreg = VLEN / sew;
    vmi.rs = _microIdx * elem_num_per_vreg;
    vmi.re = (_microIdx+1) * elem_num_per_vreg;
    %(set_reg_idx_arr)s;
    _numSrcRegs = 0;
    _numDestRegs = 0;
    %(set_dest_reg_idx)s;
    %(set_src_reg_idx)s;
}

}};

def template VectorIntMicroExecute {{

template <typename ElemType>
Fault
%(class_name)s<ElemType>::execute(ExecContext* xc,
                                  Trace::InstRecord* traceData) const
{
    using vu [[maybe_unused]] = std::make_unsigned_t<ElemType>;
    using vi [[maybe_unused]] = std::make_signed_t<ElemType>;

    if (machInst.vill) {
        return std::make_shared<IllegalInstFault>("VILL is set", machInst);
    }
    uint32_t elem_num_per_vreg = VLEN / sew;

    %(op_decl)s;
    %(op_rd)s;
    %(vm_decl_rd)s;
    COPY_OLD_VD();
    %(code)s;
    %(op_wb)s;

    return NoFault;
}

}};

def template VectorIntExtMacroDeclare {{

template<typename ElemType>
class %(class_name)s : public %(base_class)s {
private:
    %(reg_idx_arr_decl)s;
public:
    %(class_name)s(ExtMachInst _machInst);
    std::string generateDisassembly(Addr pc,
        const loader::SymbolTable *symtab) const override
    {
        std::stringstream ss;
        ss << mnemonic << ' ' << registerName(destRegIdx(0)) << ", "
            << registerName(srcRegIdx(0));
        if (machInst.vm == 0) ss << ", v0.t";
        return ss.str();
    }
};

}};

def template VectorIntExtMicroDeclare {{

template<typename ElemType>
class %(class_name)s : public %(base_class)s
{
private:
    RegId srcRegIdxArr[8];
    RegId destRegIdxArr[1];
public:
    %(class_name)s(ExtMachInst _machInst, uint8_t _microIdx);
    Fault execute(ExecContext* xc, Trace::InstRecord* traceData)const override;
    std::string generateDisassembly(Addr pc,
        const loader::SymbolTable *symtab) const override
    {
        std::stringstream ss;
        ss << mnemonic << ' ' << registerName(destRegIdx(0)) << ", "
            << registerName(srcRegIdx(0));
        if (machInst.vm == 0) ss << ", v0.t";
        return ss.str();
    }
};

}};

def template VectorIntExtMicroExecute {{

template <typename ElemType>
Fault
%(class_name)s<ElemType>::execute(ExecContext* xc,
                                  Trace::InstRecord* traceData) const
{
    using vu [[maybe_unused]] = std::make_unsigned_t<ElemType>;
    using vi [[maybe_unused]] = std::make_signed_t<ElemType>;

    if (machInst.vill) {
        return std::make_shared<IllegalInstFault>("VILL is set", machInst);
    }

    auto offset = (VLEN / sew) * (microIdx % %(ext_div)d);
    uint32_t elem_num_per_vreg = VLEN / sew;
    switch (sew / %(ext_div)d) {
      case 8: {
        using vext  [[maybe_unused]] = int8_t;
        using vextu [[maybe_unused]] = uint8_t;
        %(op_decl)s;
        %(op_rd)s;
        %(vm_decl_rd)s;
        COPY_OLD_VD();
        %(code)s;
        %(op_wb)s;
        break;
      }
      case 16: {
        using vext  [[maybe_unused]] = int16_t;
        using vextu [[maybe_unused]] = uint16_t;
        %(op_decl)s;
        %(op_rd)s;
        %(vm_decl_rd)s;
        COPY_OLD_VD();
        %(code)s;
        %(op_wb)s;
        break;
      }
      case 32: {
        using vext  [[maybe_unused]] = int32_t;
        using vextu [[maybe_unused]] = uint32_t;
        %(op_decl)s;
        %(op_rd)s;
        %(vm_decl_rd)s;
        COPY_OLD_VD();
        %(code)s;
        %(op_wb)s;
      break;
      }
      default: break;
    }

    return NoFault;
}

}};

def template VectorIntDecodeBlock {{

switch(machInst.vtype8.vsew) {
case 0b000: return new %(class_name)s<uint8_t>(machInst);
case 0b001: return new %(class_name)s<uint16_t>(machInst);
case 0b010: return new %(class_name)s<uint32_t>(machInst);
case 0b011: return new %(class_name)s<uint64_t>(machInst);
default: GEM5_UNREACHABLE;
}

}};

def template VectorIntWideningMacroDeclare {{

template<typename ElemType>
class %(class_name)s : public %(base_class)s {
private:
    %(reg_idx_arr_decl)s;
public:
    %(class_name)s(ExtMachInst _machInst);
    using %(base_class)s::generateDisassembly;
};

}};

def template VectorIntWideningMacroConstructor {{

template<typename ElemType>
%(class_name)s<ElemType>::%(class_name)s(ExtMachInst _machInst)
    : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s)
{
    // Todo: move to Decode template
    panic_if(vlmul == 3, "LMUL=8 is illegal for widening inst");
    // when LMUL setted as m1, need to split to 2 micro insts
    const uint32_t num_microops = 1 << std::max<int64_t>(0, vlmul + 1);

    StaticInstPtr microop;

    uint32_t vlmax = VLEN / sew * vflmul;
    for (int i = 0; i < num_microops; ++i) {
        microop = new %(class_name)sMicro<ElemType>(_machInst, i);
        microop->setDelayedCommit();
        this->microops.push_back(microop);
    }

    this->microops.front()->setFirstMicroop();
    this->microops.back()->setLastMicroop();
}

}};

def template VectorIntWideningMicroDeclare {{

template<typename ElemType>
class %(class_name)s : public %(base_class)s
{
private:
    // vs1, vs2, vs3(old_vd), vm for *.vv, *.vx
    RegId srcRegIdxArr[8];
    RegId destRegIdxArr[1];
public:
    %(class_name)s(ExtMachInst _machInst, uint8_t _microIdx);
    Fault execute(ExecContext* xc, Trace::InstRecord* traceData)const override;
    using %(base_class)s::generateDisassembly;
};

}};

def template VectorIntWideningMicroConstructor {{

template<typename ElemType>
%(class_name)s<ElemType>::%(class_name)s(ExtMachInst _machInst,
        uint8_t _microIdx)
    : %(base_class)s("%(mnemonic)s", _machInst,
                     %(op_class)s, _microIdx)
{
    uint32_t elem_num_per_vreg = VLEN / sew;
    vmi.rs = _microIdx * elem_num_per_vreg;
    vmi.re = (_microIdx+1) * elem_num_per_vreg;
    %(set_reg_idx_arr)s;
    _numSrcRegs = 0;
    _numDestRegs = 0;
    %(set_dest_reg_idx)s;
    %(set_src_reg_idx)s;
}

}};

def template VectorIntWideningMicroExecute {{

template <typename ElemType>
Fault
%(class_name)s<ElemType>::execute(ExecContext* xc,
                                  Trace::InstRecord* traceData) const
{
    using vu [[maybe_unused]] = std::make_unsigned_t<ElemType>;
    using vi [[maybe_unused]] = std::make_signed_t<ElemType>;
    using vwu [[maybe_unused]] = typename double_width<vu>::type;
    using vwi [[maybe_unused]] = typename double_width<vi>::type;

    if (machInst.vill) {
        return std::make_shared<IllegalInstFault>("VILL is set", machInst);
    }

    const int32_t t_micro_vlmax = VLEN / sew * (vflmul > 0 ? 1 : vflmul);
    const int32_t micro_vlmax = vlmul < 0 ? t_micro_vlmax : t_micro_vlmax / 2;
    [[maybe_unused]] const size_t offset =
        (this->microIdx % 2 == 0) ? 0 : micro_vlmax;
    uint32_t elem_num_per_vreg = micro_vlmax;

    %(op_decl)s;
    %(op_rd)s;
    %(vm_decl_rd)s;
    COPY_OLD_VD();
    %(code)s;
    %(op_wb)s;
    return NoFault;
}

}};

def template VectorIntNarrowingMicroExecute {{

template <typename ElemType>
Fault
%(class_name)s<ElemType>::execute(ExecContext* xc,
                                  Trace::InstRecord* traceData) const
{
    using vu [[maybe_unused]] = std::make_unsigned_t<ElemType>;
    using vi [[maybe_unused]] = std::make_signed_t<ElemType>;
    using vwu [[maybe_unused]] = typename double_width<vu>::type;
    using vwi [[maybe_unused]] = typename double_width<vi>::type;

    if (machInst.vill) {
        return std::make_shared<IllegalInstFault>("VILL is set", machInst);
    }
    const int32_t t_micro_vlmax = VLEN / sew * (vflmul > 0 ? 1 : vflmul);
    const int32_t micro_vlmax = vlmul < 0 ? t_micro_vlmax : t_micro_vlmax / 2;
    [[maybe_unused]] const size_t offset =
        (this->microIdx % 2 == 0) ? 0 : micro_vlmax;
    uint32_t elem_num_per_vreg = micro_vlmax;

    %(op_decl)s;
    %(op_rd)s;
    %(vm_decl_rd)s;
    COPY_OLD_VD();
    %(code)s;
    %(op_wb)s;
    return NoFault;
}

}};

def template VectorIntWideningDecodeBlock {{

switch(machInst.vtype8.vsew) {
case 0b000: return new %(class_name)s<uint8_t>(machInst);
case 0b001: return new %(class_name)s<uint16_t>(machInst);
case 0b010: return new %(class_name)s<uint32_t>(machInst);
default: GEM5_UNREACHABLE;
}

}};

def template VectorFloatMacroDeclare {{

template<typename ElemType>
class %(class_name)s : public %(base_class)s {
private:
    %(reg_idx_arr_decl)s;
public:
    %(class_name)s(ExtMachInst _machInst);
    using %(base_class)s::generateDisassembly;
};

}};

def template VectorFloatMacroConstructor {{
template<typename ElemType>
%(class_name)s<ElemType>::%(class_name)s(ExtMachInst _machInst)
    : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s)
{
    StaticInstPtr microop;

    const uint32_t num_microops = (vflmul < 1 ? 1 : vflmul);
    for (int i = 0; i < num_microops; ++i) {
        microop = new %(class_name)sMicro<ElemType>(_machInst, i);
        microop->setDelayedCommit();
        this->microops.push_back(microop);
    }

    this->microops.front()->setFirstMicroop();
    this->microops.back()->setLastMicroop();
}
}};

def template VectorFloatMicroDeclare {{

template<typename ElemType>
class %(class_name)s : public %(base_class)s
{
private:
    // vs1, vs2, vs3(old_vd), vm
    RegId srcRegIdxArr[8];
    RegId destRegIdxArr[1];
public:
    %(class_name)s(ExtMachInst _machInst, uint8_t _microIdx);
    Fault execute(ExecContext* xc, Trace::InstRecord* traceData)const override;
    using %(base_class)s::generateDisassembly;
};

}};

def template VectorFloatMicroConstructor {{
template<typename ElemType>
%(class_name)s<ElemType>::%(class_name)s(ExtMachInst _machInst,
                                         uint8_t _microIdx)
    : %(base_class)s("%(mnemonic)s", _machInst,
                     %(op_class)s, _microIdx)
{
    uint32_t elem_num_per_vreg = VLEN / sew;
    vmi.rs = _microIdx * elem_num_per_vreg;
    vmi.re = (_microIdx+1) * elem_num_per_vreg;
    %(set_reg_idx_arr)s;
    _numSrcRegs = 0;
    _numDestRegs = 0;
    %(set_dest_reg_idx)s;
    %(set_src_reg_idx)s;
}

}};

def template VectorFloatMicroExecute {{

template <typename ElemType>
Fault
%(class_name)s<ElemType>::execute(ExecContext* xc,
                                  Trace::InstRecord* traceData) const
{
    using et = ElemType;
    using vu = decltype(et::v);
    if (machInst.vill) {
        return std::make_shared<IllegalInstFault>("VILL is set", machInst);
    }
    uint32_t elem_num_per_vreg = VLEN / sew;

    VRM_REQUIRED;

    %(op_decl)s;
    %(op_rd)s;
    %(vm_decl_rd)s;
    COPY_OLD_VD();
    %(code)s;
    %(op_wb)s;

    return NoFault;
}

}};

def template VectorFloatDecodeBlock {{

switch(machInst.vtype8.vsew) {
case 0b010: return new %(class_name)s<float32_t>(machInst);
case 0b011: return new %(class_name)s<float64_t>(machInst);
default: GEM5_UNREACHABLE;
}

}};

def template VectorFloatCvtMacroDeclare {{

template<typename ElemType>
class %(class_name)s : public %(base_class)s {
private:
    %(reg_idx_arr_decl)s;
public:
    %(class_name)s(ExtMachInst _machInst);
    std::string generateDisassembly(Addr pc,
        const loader::SymbolTable *symtab) const override
    {
        std::stringstream ss;
        ss << mnemonic << ' ' << registerName(destRegIdx(0)) << ", "
            << registerName(srcRegIdx(0));
        if (machInst.vm == 0) ss << ", v0.t";
        return ss.str();
    }
};

}};

def template VectorFloatCvtMicroDeclare {{

template<typename ElemType>
class %(class_name)s : public %(base_class)s
{
private:
    RegId srcRegIdxArr[8];
    RegId destRegIdxArr[1];
public:
    %(class_name)s(ExtMachInst _machInst, uint8_t _microIdx);
    Fault execute(ExecContext* xc, Trace::InstRecord* traceData)const override;
    std::string generateDisassembly(Addr pc,
        const loader::SymbolTable *symtab) const override
    {
        std::stringstream ss;
        ss << mnemonic << ' ' << registerName(destRegIdx(0)) << ", "
            << registerName(srcRegIdx(0));
        if (machInst.vm == 0) ss << ", v0.t";
        return ss.str();
    }
};

}};


def template VectorFloatWideningMicroExecute {{

template <typename ElemType>
Fault
%(class_name)s<ElemType>::execute(ExecContext* xc,
                                  Trace::InstRecord* traceData) const
{
    using et = ElemType;
    using vu [[maybe_unused]] = decltype(et::v);
    using ewt = typename double_width<et>::type;
    using vwu = decltype(ewt::v);

    if (machInst.vill) {
        return std::make_shared<IllegalInstFault>("VILL is set", machInst);
    }

    VRM_REQUIRED;

    const int32_t t_micro_vlmax = VLEN / sew * (vflmul > 0 ? 1 : vflmul);
    const int32_t micro_vlmax = vlmul < 0 ? t_micro_vlmax : t_micro_vlmax / 2;
    [[maybe_unused]] const size_t offset =
        (this->microIdx % 2 == 0) ? 0 : micro_vlmax;
    uint32_t elem_num_per_vreg = micro_vlmax;

    %(op_decl)s;
    %(op_rd)s;
    %(vm_decl_rd)s;
    COPY_OLD_VD();
    %(code)s;
    %(op_wb)s;
    return NoFault;
}

}};

def template VectorFloatNarrowingMicroExecute {{

template <typename ElemType>
Fault
%(class_name)s<ElemType>::execute(ExecContext* xc,
                                  Trace::InstRecord* traceData) const
{
    using et = ElemType;
    using vu [[maybe_unused]] = decltype(et::v);
    using ewt = typename double_width<et>::type;
    using vwu = decltype(ewt::v);

    if (machInst.vill) {
        return std::make_shared<IllegalInstFault>("VILL is set", machInst);
    }

    VRM_REQUIRED;

    const int32_t t_micro_vlmax = VLEN / sew * (vflmul > 0 ? 1 : vflmul);
    const int32_t micro_vlmax = vlmul < 0 ? t_micro_vlmax : t_micro_vlmax / 2;
    [[maybe_unused]] const size_t offset =
        (this->microIdx % 2 == 0) ? 0 : micro_vlmax;
    uint32_t elem_num_per_vreg = micro_vlmax;

    %(op_decl)s;
    %(op_rd)s;
    %(vm_decl_rd)s;
    COPY_OLD_VD();
    %(code)s;
    %(op_wb)s;
    return NoFault;
}

}};

def template VectorFloatWideningDecodeBlock {{

switch(machInst.vtype8.vsew) {
case 0b010: return new %(class_name)s<float32_t>(machInst);
default: GEM5_UNREACHABLE;
}

}};

def template ViotaMacroDeclare {{

template<typename ElemType>
class %(class_name)s : public %(base_class)s {
private:
    RegId srcRegIdxArr[8];
    RegId destRegIdxArr[2];
public:
    %(class_name)s(ExtMachInst _machInst);
    using %(base_class)s::generateDisassembly;
};

}};


def template ViotaMacroConstructor {{

template<typename ElemType>
%(class_name)s<ElemType>::%(class_name)s(ExtMachInst _machInst)
    : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s)
{
    const uint32_t elem_num_per_vreg = VLEN / sew;
    const uint32_t num_microops = (vflmul < 1 ? 1 : vflmul);

    StaticInstPtr microop;

    // Allow one empty micro op to hold IsLastMicroop flag
    VectorMicroInfo vmi;
    for (int i = 0; i < num_microops; ++i) {
        vmi.rs = i * elem_num_per_vreg;
        vmi.re = (i+1) * elem_num_per_vreg;
        vmi.microVd = VD + i;
        if (vmi.microVd >= 32) {
            break;
        }
        microop = new %(class_name)sMicro<ElemType>(_machInst, i, vmi);
        microop->setDelayedCommit();
        this->microops.push_back(microop);
    }

    this->microops.front()->setFirstMicroop();
    this->microops.back()->setLastMicroop();
}

}};

def template ViotaMicroDeclare {{

template<typename ElemType>
class %(class_name)s : public %(base_class)s
{
private:
    RegId srcRegIdxArr[8];
    RegId destRegIdxArr[2];
public:
    %(class_name)s(ExtMachInst _machInst, uint8_t _microIdx, VectorMicroInfo& _vmi);
    Fault execute(ExecContext* xc, Trace::InstRecord* traceData)const override;
    using %(base_class)s::generateDisassembly;
};

}};

def template ViotaMicroConstructor {{

template<typename ElemType>
%(class_name)s<ElemType>::%(class_name)s(ExtMachInst _machInst, uint8_t _microIdx, VectorMicroInfo& _vmi)
    : %(base_class)s("%(mnemonic)s", _machInst,
                     %(op_class)s, _microIdx)
{
    %(set_reg_idx_arr)s;
    vmi = _vmi;
    assert(~vmi.microVd);
    _numDestRegs = 0;
    _numSrcRegs = 0;
    setDestRegIdx(_numDestRegs++, RegId(VecRegClass, vmi.microVd));
    _numTypedDestRegs[VecRegClass]++;
    setDestRegIdx(_numDestRegs++, RegId(VecRegClass, VecTempReg0)); // has_one cnt
    _numTypedDestRegs[VecRegClass]++;

    setSrcRegIdx(_numSrcRegs++, RegId(VecRegClass, _machInst.vs2));
    SET_VL_SRC();
    SET_OLDDST_SRC();
    if (_microIdx != 0) {
        setSrcRegIdx(_numSrcRegs++, RegId(VecRegClass, VecTempReg0));
    }
    SET_VM_SRC();

}

}};

def template ViotaMicroExecute {{

template <typename ElemType>
Fault
%(class_name)s<ElemType>::execute(ExecContext* xc,
                                  Trace::InstRecord* traceData) const
{
    using vu [[maybe_unused]] = std::make_unsigned_t<ElemType>;
    using vi [[maybe_unused]] = std::make_signed_t<ElemType>;
    if (machInst.vill) {
        return std::make_shared<IllegalInstFault>("VILL is set", machInst);
    }
    uint32_t elem_num_per_vreg = VLEN / sew;

    %(op_decl)s;
    %(op_rd)s;
    %(vm_decl_rd)s;
    COPY_OLD_VD()
    %(code)s;
    %(op_wb)s;
    return NoFault;
}

}};


def template Vector1Vs1VdMaskConstructor {{

template<typename ElemType>
%(class_name)s<ElemType>::%(class_name)s(ExtMachInst _machInst)
    : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s)
{
    %(set_reg_idx_arr)s;
    %(set_dest_reg_idx)s;
    %(set_src_reg_idx)s;
    %(set_vm_idx)s;
}

}};

def template Vector1Vs1VdMaskExecute {{

template<typename ElemType>
Fault
%(class_name)s<ElemType>::execute(ExecContext* xc,
                                  Trace::InstRecord* traceData) const
{
    using vu = uint8_t;
    if (machInst.vill) {
        return std::make_shared<IllegalInstFault>("VILL is set", machInst);
    }

    %(op_decl)s;
    %(op_rd)s;
    %(vm_decl_rd)s;
    COPY_OLD_VD();
    %(code)s;
    %(op_wb)s;
    return NoFault;
};

}};

def template Vector1Vs1RdMaskDeclare {{

template<typename ElemType>
class %(class_name)s : public %(base_class)s {
private:
    RegId srcRegIdxArr[8];
    RegId destRegIdxArr[1];
public:
    %(class_name)s(ExtMachInst _machInst);
    Fault execute(ExecContext* xc, Trace::InstRecord* traceData)const override;
    using %(base_class)s::generateDisassembly;
};

}};

def template Vector1Vs1RdMaskConstructor {{

template<typename ElemType>
%(class_name)s<ElemType>::%(class_name)s(ExtMachInst _machInst)
    : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s)
{
    %(set_reg_idx_arr)s;
    %(constructor)s;
    %(set_vm_idx)s;
}

}};

def template Vector1Vs1RdMaskExecute {{

template<typename ElemType>
Fault
%(class_name)s<ElemType>::execute(ExecContext* xc,
                                  Trace::InstRecord* traceData) const
{
    using vu [[maybe_unused]] = std::make_unsigned_t<ElemType>;
    using vi [[maybe_unused]] = std::make_signed_t<ElemType>;
    if (machInst.vill) {
        return std::make_shared<IllegalInstFault>("VILL is set", machInst);
    }

    %(op_rd)s;
    uint64_t Rd = 0;
    %(vm_decl_rd)s;
    %(code)s;
    %(op_wb)s;
    return NoFault;
};

}};

def template VectorIntMaskMacroDeclare {{

template<typename ElemType>
class %(class_name)s : public %(base_class)s {
private:
    %(reg_idx_arr_decl)s;
public:
    %(class_name)s(ExtMachInst _machInst);
    using %(base_class)s::generateDisassembly;
};

}};

def template VectorIntMaskMacroConstructor {{

template<typename ElemType>
%(class_name)s<ElemType>::%(class_name)s(ExtMachInst _machInst)
    : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s)
{
    const uint32_t num_microops = vflmul < 1 ? 1 : vflmul;
    StaticInstPtr microop;

    for (int i = 0; i < num_microops; ++i) {
        microop = new %(class_name)sMicro<ElemType>(_machInst, i);
        microop->setDelayedCommit();
        this->microops.push_back(microop);
    }

    microop = new VMaskMergeMicroInst<ElemType>(_machInst, _machInst.vd,
        this->microops.size());
    this->microops.push_back(microop);

    this->microops.front()->setFirstMicroop();
    this->microops.back()->setLastMicroop();
}

}};

def template VectorIntMaskMicroDeclare {{

template<typename ElemType>
class %(class_name)s : public %(base_class)s
{
private:
    // vs1(rs1), vs2, old_vd, v0 for *.vv[m] or *.vx[m]
    // vs2, old_vd, v0 for *.vi[m]
    RegId srcRegIdxArr[8];
    RegId destRegIdxArr[1];
public:
    %(class_name)s(ExtMachInst _machInst,
                uint8_t _microIdx);
    Fault execute(ExecContext* xc, Trace::InstRecord* traceData)const override;
    using %(base_class)s::generateDisassembly;
};

}};

def template VectorIntMaskMicroConstructor {{

template<typename ElemType>
%(class_name)s<ElemType>::%(class_name)s(ExtMachInst _machInst,
                                         uint8_t _microIdx)
: %(base_class)s("%(mnemonic)s", _machInst,
                 %(op_class)s, _microIdx)
{
    uint32_t elem_num_per_vreg = VLEN / sew;
    vmi.rs = _microIdx * elem_num_per_vreg;
    vmi.re = (_microIdx+1) * elem_num_per_vreg;
    %(set_reg_idx_arr)s;
    _numSrcRegs = 0;
    _numDestRegs = 0;
    %(set_dest_reg_idx)s;
    %(set_src_reg_idx)s;
}

}};

def template VectorIntMaskMicroExecute {{

template <typename ElemType>
Fault
%(class_name)s<ElemType>::execute(ExecContext* xc,
                                  Trace::InstRecord* traceData) const
{
    using vu [[maybe_unused]] = std::make_unsigned_t<ElemType>;
    using vi [[maybe_unused]] = std::make_signed_t<ElemType>;
    if (machInst.vill) {
        return std::make_shared<IllegalInstFault>("VILL is set", machInst);
    }

    %(op_decl)s;
    %(op_rd)s;
    %(vm_decl_rd)s;
    COPY_OLD_VD();

    const uint16_t bit_offset = VLEN / sew;
    const uint16_t offset = bit_offset * microIdx;
    uint32_t elem_num_per_vreg = VLEN / sew;

    %(code)s;
    %(op_wb)s;
    return NoFault;
}

}};

def template VectorFloatMaskMacroDeclare {{

template<typename ElemType>
class %(class_name)s : public %(base_class)s {
private:
    %(reg_idx_arr_decl)s;
public:
    %(class_name)s(ExtMachInst _machInst);
    using %(base_class)s::generateDisassembly;
};

}};

def template VectorFloatMaskMacroConstructor {{

template<typename ElemType>
%(class_name)s<ElemType>::%(class_name)s(ExtMachInst _machInst)
    : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s)
{
    const uint32_t num_microops = vflmul < 1 ? 1 : vflmul;

    StaticInstPtr microop;

    for (int i = 0; i < num_microops; ++i) {
        microop = new %(class_name)sMicro<ElemType>(_machInst, i);
        microop->setDelayedCommit();
        this->microops.push_back(microop);
    }

    microop = new VMaskMergeMicroInst<ElemType>(_machInst, _machInst.vd,
        this->microops.size());
    this->microops.push_back(microop);

    this->microops.front()->setFirstMicroop();
    this->microops.back()->setLastMicroop();
}

}};

def template VectorFloatMaskMicroDeclare {{

template<typename ElemType>
class %(class_name)s : public %(base_class)s
{
private:
    // vs1(rs1), vs2, old_vd, v0 for *.vv or *.vf
    RegId srcRegIdxArr[8];
    RegId destRegIdxArr[1];
public:
    %(class_name)s(ExtMachInst _machInst,
                    uint8_t _microIdx);
    Fault execute(ExecContext* xc, Trace::InstRecord* traceData)const override;
    using %(base_class)s::generateDisassembly;
};

}};

def template VectorFloatMaskMicroConstructor {{

template<typename ElemType>
%(class_name)s<ElemType>::%(class_name)s(ExtMachInst _machInst,
                                          uint8_t _microIdx)
: %(base_class)s("%(mnemonic)s", _machInst,
                 %(op_class)s, _microIdx)
{
    uint32_t elem_num_per_vreg = VLEN / sew;
    vmi.rs = _microIdx * elem_num_per_vreg;
    vmi.re = (_microIdx+1) * elem_num_per_vreg;
    %(set_reg_idx_arr)s;
    _numSrcRegs = 0;
    _numDestRegs = 0;
    %(set_dest_reg_idx)s;
    %(set_src_reg_idx)s;
}

}};

def template VectorFloatMaskMicroExecute {{

template <typename ElemType>
Fault
%(class_name)s<ElemType>::execute(ExecContext* xc,
                                  Trace::InstRecord* traceData) const
{
    using et = ElemType;
    using vu = decltype(et::v);
    if (machInst.vill) {
        return std::make_shared<IllegalInstFault>("VILL is set", machInst);
    }
    uint32_t elem_num_per_vreg = VLEN / sew;

    %(op_decl)s;
    %(op_rd)s;
    %(vm_decl_rd)s;
    COPY_OLD_VD();

    const uint16_t bit_offset = VLENB / sizeof(ElemType);
    const uint16_t offset = bit_offset * microIdx;

    %(code)s;
    %(op_wb)s;
    return NoFault;
}

}};

def template VMvWholeMacroDeclare {{

class %(class_name)s : public %(base_class)s {
private:
    %(reg_idx_arr_decl)s;
public:
    %(class_name)s(ExtMachInst _machInst);
    using %(base_class)s::generateDisassembly;
};

}};

def template VMvWholeMacroConstructor {{

%(class_name)s::%(class_name)s(ExtMachInst _machInst)
    : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s)
{
    const uint32_t num_microops = _machInst.simm3 + 1;
    StaticInstPtr microop;

    for (int i = 0; i < num_microops; ++i) {
        microop = new %(class_name)sMicro(_machInst, i);
        microop->setDelayedCommit();
        this->microops.push_back(microop);
    }

    this->microops.front()->setFirstMicroop();
    this->microops.back()->setLastMicroop();
}

}};

def template VMvWholeMicroDeclare {{

class %(class_name)s : public %(base_class)s
{
private:
    RegId srcRegIdxArr[8];
    RegId destRegIdxArr[1];
public:
    %(class_name)s(ExtMachInst _machInst,
                   uint8_t _microIdx);
    Fault execute(ExecContext* xc, Trace::InstRecord* traceData)const override;
    using %(base_class)s::generateDisassembly;
};

}};

def template VMvWholeMicroConstructor {{

%(class_name)s::%(class_name)s(ExtMachInst _machInst,
                               uint8_t _microIdx)
    : %(base_class)s("%(mnemonic)s", _machInst,
                     %(op_class)s, _microIdx)
{
    uint32_t elem_num_per_vreg = VLEN / sew;
    vmi.rs = _microIdx * elem_num_per_vreg;
    vmi.re = (_microIdx+1) * elem_num_per_vreg;
    %(set_reg_idx_arr)s;
    _numSrcRegs = 0;
    _numDestRegs = 0;
    setDestRegIdx(_numDestRegs++, RegId(VecRegClass, _machInst.vd + _microIdx));
    _numTypedDestRegs[VecRegClass]++;
    setSrcRegIdx(_numSrcRegs++, RegId(VecRegClass, _machInst.vs2 + _microIdx));
    SET_OLDDST_SRC();
}

}};

def template VMvWholeMicroExecute {{

Fault
%(class_name)s::execute(ExecContext* xc, Trace::InstRecord* traceData) const
{
    // TODO: Check register alignment.
    // TODO: If vd is equal to vs2 the instruction is an architectural NOP.
    %(op_decl)s;
    %(op_rd)s;
    uint32_t elem_num_per_vreg = VLEN / sew;

    for (size_t i = 0; i < (VLEN / 64); i++) {
        %(code)s;
    }
    %(op_wb)s;
    return NoFault;
}

}};

def template VectorMaskDeclare {{

template<typename ElemType>
class %(class_name)s : public %(base_class)s {
private:
    RegId srcRegIdxArr[8];
    RegId destRegIdxArr[1];
public:
    %(class_name)s(ExtMachInst _machInst);
    Fault execute(ExecContext* xc, Trace::InstRecord* traceData)const override;
    using %(base_class)s::generateDisassembly;
};

}};

def template VectorMaskConstructor {{

template<typename ElemType>
%(class_name)s<ElemType>::%(class_name)s(ExtMachInst _machInst)
    : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s)
{
    %(set_reg_idx_arr)s;
    %(set_dest_reg_idx)s;
    %(set_src_reg_idx)s;
}

}};

def template VectorMaskExecute {{

template<typename ElemType>
Fault
%(class_name)s<ElemType>::execute(ExecContext* xc,
                                  Trace::InstRecord* traceData) const
{
    using vu = uint8_t;

    if (machInst.vill) {
        return std::make_shared<IllegalInstFault>("VILL is set", machInst);
    }

    %(op_decl)s;
    %(op_rd)s;
    // TODO: remove it
    COPY_OLD_VD();
    %(code)s;
    %(op_wb)s;

    return NoFault;
};

}};

def template VectorMaskDecodeBlock {{

return new %(class_name)s<uint8_t>(machInst);

}};

def template VectorNonSplitDeclare {{

template<typename ElemType>
class %(class_name)s : public %(base_class)s {
private:
    RegId srcRegIdxArr[8];
    RegId destRegIdxArr[1];
public:
    %(class_name)s(ExtMachInst _machInst);
    Fault execute(ExecContext* xc, Trace::InstRecord* traceData)const override;
    using %(base_class)s::generateDisassembly;
};

}};

def template VectorNonSplitConstructor {{

template<typename ElemType>
%(class_name)s<ElemType>::%(class_name)s(ExtMachInst _machInst)
    : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s)
{
    vmi.rs = 0;
    vmi.re = 1;
    %(set_reg_idx_arr)s;
    %(constructor)s;
    SET_OLDDST_SRC();
    %(set_vm_idx)s;
}

}};

def template VectorIntNonSplitExecute {{

template <typename ElemType>
Fault
%(class_name)s<ElemType>::execute(ExecContext* xc,
                                    Trace::InstRecord* traceData) const
{
    using vu [[maybe_unused]] = std::make_unsigned_t<ElemType>;
    using vi [[maybe_unused]] = std::make_signed_t<ElemType>;
    if (machInst.vill) {
        return std::make_shared<IllegalInstFault>("VILL is set", machInst);
    }
    %(op_decl)s;
    %(op_rd)s;
    %(vm_decl_rd)s;
    %(code)s;
    %(op_wb)s;
    return NoFault;
}

}};

def template VectorFloatNonSplitExecute {{

template <typename ElemType>
Fault
%(class_name)s<ElemType>::execute(ExecContext* xc,
                                    Trace::InstRecord* traceData) const
{
    using et = ElemType;
    using vu = decltype(et::v);

    if (machInst.vill) {
        return std::make_shared<IllegalInstFault>("VILL is set", machInst);
    }
    %(op_decl)s;
    %(op_rd)s;
    %(vm_decl_rd)s;
    %(code)s;
    %(op_wb)s;
    return NoFault;
}

}};

def template VectorReduceMacroDeclare {{

template<typename ElemType>
class %(class_name)s : public %(base_class)s {
public:
    %(class_name)s(ExtMachInst _machInst);
    using %(base_class)s::generateDisassembly;
};

}};

def template VectorReduceMacroConstructor {{

template<typename ElemType>
%(class_name)s<ElemType>::%(class_name)s(ExtMachInst _machInst)
    : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s)
{
    const uint32_t num_microops = vflmul < 1 ? 1 : vflmul;

    StaticInstPtr microop;

    for (int i = 0; i < num_microops; ++i) {
        microop = new %(class_name)sMicro<ElemType>(_machInst, i, (i == 0 ? 0 : 1));
        microop->setDelayedCommit();
        this->microops.push_back(microop);
    }
    microop = new %(class_name)sMicro<ElemType>(_machInst, num_microops, 2);
    microop->setDelayedCommit();
    this->microops.push_back(microop);
    this->microops.front()->setFirstMicroop();
    this->microops.back()->setLastMicroop();
}

}};

def template VectorReduceMicroDeclare {{

template<typename ElemType>
class %(class_name)s : public %(base_class)s
{
private:
    // first microop: vtmp0 <- reduce_loop(vs2)
    // middle microop: vtmp0 <- vtmp0 + reduce_loop(vs2)
    // last microop: vd <= oldvd + vtmp0 + vs1
    // vs2, vs1, vd, vm
    RegId srcRegIdxArr[8];
    RegId destRegIdxArr[1];
public:
    enum MicroopPos {
        FirstUop = 0,  // 0
        MiddleUop = 1, // 1
        LastUop = 2,   // 2
    } pos;
    %(class_name)s(ExtMachInst _machInst,
                   uint8_t _microIdx, int pos);
    Fault execute(ExecContext* xc, Trace::InstRecord* traceData)const override;
    std::string generateDisassembly(Addr pc,
        const Loader::SymbolTable *symtab) const override
    {
        std::stringstream ss;
        if (pos==FirstUop) {
            ss << mnemonic << ' ' << registerName(destRegIdx(0)) << ", ";
            ss << registerName(srcRegIdx(0));
        }
        if (pos==MiddleUop) {
            ss << mnemonic << ' ' << registerName(destRegIdx(0)) << ", ";
            ss << registerName(srcRegIdx(1)) << "," << registerName(srcRegIdx(0));
        }
        if (pos==LastUop) {
            ss << mnemonic << ' ' << registerName(destRegIdx(0)) << ", ";
            ss << registerName(srcRegIdx(2)) << "," << registerName(srcRegIdx(1));
        }

        if (machInst.vm == 0) ss << ", v0.t";
        return ss.str();
    }
};

}};

def template VectorReduceMicroConstructor {{

template<typename ElemType>
%(class_name)s<ElemType>::%(class_name)s(ExtMachInst _machInst,
                                         uint8_t _microIdx, int pos)
: %(base_class)s("%(mnemonic)s", _machInst,
                 %(op_class)s, _microIdx)
{
    this->pos = (MicroopPos)pos;
    uint32_t elem_num_per_vreg = VLEN / sew;
    vmi.rs = 0;
    vmi.re = 1;
    %(set_reg_idx_arr)s;
    _numSrcRegs = 0;
    _numDestRegs = 0;
    %(set_dest_reg_idx)s;
    %(set_src_reg_idx)s;
}

}};

def template VectorReduceIntMicroExecute {{

template <typename ElemType>
Fault
%(class_name)s<ElemType>::execute(ExecContext* xc,
                                  Trace::InstRecord* traceData) const
{
    %(type_def)s;
    if (machInst.vill) {
        return std::make_shared<IllegalInstFault>("VILL is set", machInst);
    }

    auto &tmp_d0 =
    *(RiscvISAInst::VecRegContainer *)
    xc->getWritableRegOperand(
        this, 0);
    auto Vd = tmp_d0.as<vui>();

    uint64_t rVl = 0;
    assert(vlsrcIdx > 0);
    rVl = xc->getRegOperand(this, vlsrcIdx);

    uint32_t microvlmax = VLEN / sew * (vflmul > 0 ? 1 : vflmul);
    uint32_t elem_num_per_vreg = microvlmax;

    %(vm_decl_rd)s;
    vui vtmp, vs1;
    vui *Vs2=nullptr;

    auto func = %(code)s;
    auto reduce_loop =
        [&, this](bool middle, vui vtmp) {
            ElemType microop_result = middle ? vtmp : Vs2[0];
            for (uint32_t i = 0; i < microvlmax; i++) {
                uint32_t ei = i + microvlmax * this->microIdx;
                if ((ei < rVl) && (this->vm || elem_mask(v0, ei))) {
                    microop_result = func(microop_result, Vs2[i]);
                } else if (ei >= rVl) {
                    break;
                }
            }
            return microop_result;
        };

    vui result;
    if (pos == FirstUop) {
        RiscvISAInst::VecRegContainer tmp_s0;
        xc->getRegOperand(this, 0,
            &tmp_s0);
        Vs2 = tmp_s0.as<vui>();
        result = reduce_loop(false, 0);
    } else if (pos == MiddleUop) {
        RiscvISAInst::VecRegContainer tmp_s0;
        xc->getRegOperand(this, 0,
            &tmp_s0);
        vtmp = tmp_s0.as<vui>()[0];

        xc->getRegOperand(this, 1,
            &tmp_s0);
        Vs2 = tmp_s0.as<vui>();
        result = reduce_loop(true, vtmp);
    } else {
        COPY_OLD_VD();
        RiscvISAInst::VecRegContainer tmp_s0;
        xc->getRegOperand(this, 1,
            &tmp_s0);
        vtmp = tmp_s0.as<vui>()[0];
        xc->getRegOperand(this, 2,
            &tmp_s0);
        vs1 = tmp_s0.as<vui>()[0];
        result = func(vtmp, vs1);
    }
    Vd[0] = result;

    xc->setRegOperand(this,0, &tmp_d0);
    if (traceData) {
        traceData->setData(tmp_d0);
    }

    return NoFault;
}

}};

def template VectorReduceFloatMicroExecute {{

template <typename ElemType>
Fault
%(class_name)s<ElemType>::execute(ExecContext* xc,
                                  Trace::InstRecord* traceData) const
{
    %(type_def)s;
    if (machInst.vill) {
        return std::make_shared<IllegalInstFault>("VILL is set", machInst);
    }

    auto &tmp_d0 =
    *(RiscvISAInst::VecRegContainer *)
    xc->getWritableRegOperand(
        this, 0);
    auto Vd = tmp_d0.as<vu>();

    uint64_t rVl = 0;
    assert(vlsrcIdx > 0);
    rVl = xc->getRegOperand(this, vlsrcIdx);

    uint32_t microvlmax = VLEN / sew * (vflmul > 0 ? 1 : vflmul);
    uint32_t elem_num_per_vreg = microvlmax;

    %(vm_decl_rd)s;
    vu vtmp, vs1;
    vu *Vs2=nullptr;

    auto func = %(code)s;
    auto reduce_loop =
        [&, this](bool middle, vu vtmp) {
            vu microop_result = middle ? vtmp : Vs2[0];
            for (uint32_t i = 0; i < microvlmax; i++) {
                uint32_t ei = i + microvlmax * this->microIdx;
                if ((ei < rVl) && (this->vm || elem_mask(v0, ei))) {
                    microop_result = func(microop_result, Vs2[i]).v;
                } else if (ei >= rVl) {
                    break;
                }
            }
            return microop_result;
        };

    vu result;
    if (pos == FirstUop) {
        RiscvISAInst::VecRegContainer tmp_s0;
        xc->getRegOperand(this, 0,
            &tmp_s0);
        Vs2 = tmp_s0.as<vu>();
        result = reduce_loop(false, 0);
    } else if (pos == MiddleUop) {
        RiscvISAInst::VecRegContainer tmp_s0;
        xc->getRegOperand(this, 0,
            &tmp_s0);
        vtmp = tmp_s0.as<vu>()[0];

        xc->getRegOperand(this, 1,
            &tmp_s0);
        Vs2 = tmp_s0.as<vu>();
        result = reduce_loop(true, vtmp);
    } else {
        COPY_OLD_VD();
        RiscvISAInst::VecRegContainer tmp_s0;
        xc->getRegOperand(this, 1,
            &tmp_s0);
        vtmp = tmp_s0.as<vu>()[0];
        xc->getRegOperand(this, 2,
            &tmp_s0);
        vs1 = tmp_s0.as<vu>()[0];
        result = func(vtmp, vs1).v;
    }
    RegVal FFLAGS = xc->readMiscReg(MISCREG_FFLAGS);
    std::feclearexcept(FE_ALL_EXCEPT);
    FFLAGS |= softfloat_exceptionFlags;
    if (softfloat_exceptionFlags) {
        softfloat_exceptionFlags = 0;
        xc->setMiscReg(MISCREG_FFLAGS, FFLAGS);
    }

    Vd[0] = result;

    xc->setRegOperand(this,0, &tmp_d0);
    if (traceData) {
        traceData->setData(tmp_d0);
    }

    return NoFault;
}

}};

def template VectorReduceIntWideningMicroExecute {{

template <typename ElemType>
Fault
%(class_name)s<ElemType>::execute(ExecContext* xc,
                                  Trace::InstRecord* traceData) const
{
    %(type_def)s;
    if (machInst.vill) {
        return std::make_shared<IllegalInstFault>("VILL is set", machInst);
    }

    auto &tmp_d0 =
    *(RiscvISAInst::VecRegContainer *)
    xc->getWritableRegOperand(
        this, 0);
    auto Vd = tmp_d0.as<vwui>();

    uint64_t rVl = 0;
    assert(vlsrcIdx > 0);
    rVl = xc->getRegOperand(this, vlsrcIdx);

    uint32_t microvlmax = VLEN / sew * (vflmul > 0 ? 1 : vflmul);
    uint32_t elem_num_per_vreg = microvlmax;

    %(vm_decl_rd)s;
    vwui vtmp, vs1;
    vui *Vs2=nullptr;

    auto func = %(code)s;
    auto reduce_loop =
        [&, this](bool middle, vwui vtmp) {
            vwui microop_result = middle ? vtmp : Vs2[0];
            for (uint32_t i = 0; i < microvlmax; i++) {
                uint32_t ei = i + microvlmax * this->microIdx;
                if ((ei < rVl) && (this->vm || elem_mask(v0, ei))) {
                    microop_result = func(microop_result, Vs2[i]);
                } else if (ei >= rVl) {
                    break;
                }
            }
            return microop_result;
        };

    vui result;
    if (pos == FirstUop) {
        RiscvISAInst::VecRegContainer tmp_s0;
        xc->getRegOperand(this, 0,
            &tmp_s0);
        Vs2 = tmp_s0.as<vui>();
        result = reduce_loop(false, 0);
    } else if (pos == MiddleUop) {
        RiscvISAInst::VecRegContainer tmp_s0;
        xc->getRegOperand(this, 0,
            &tmp_s0);
        vtmp = tmp_s0.as<vwui>()[0];
        xc->getRegOperand(this, 1,
            &tmp_s0);
        Vs2 = tmp_s0.as<vui>();
        result = reduce_loop(true, vtmp);
    } else {
        COPY_OLD_VD();
        RiscvISAInst::VecRegContainer tmp_s0;
        xc->getRegOperand(this, 1,
            &tmp_s0);
        vtmp = tmp_s0.as<vwui>()[0];
        xc->getRegOperand(this, 2,
            &tmp_s0);
        vs1 = tmp_s0.as<vwui>()[0];
        result = func(vtmp, vs1);
    }
    Vd[0] = result;

    xc->setRegOperand(this,0, &tmp_d0);
    if (traceData) {
        traceData->setData(tmp_d0);
    }
    return NoFault;
}

}};

def template VectorReduceFloatWideningMicroExecute {{

template <typename ElemType>
Fault
%(class_name)s<ElemType>::execute(ExecContext* xc,
                                  Trace::InstRecord* traceData) const
{
    %(type_def)s;
    if (machInst.vill) {
        return std::make_shared<IllegalInstFault>("VILL is set", machInst);
    }

    auto &tmp_d0 =
    *(RiscvISAInst::VecRegContainer *)
    xc->getWritableRegOperand(
        this, 0);
    auto Vd = tmp_d0.as<vwu>();

    uint64_t rVl = 0;
    assert(vlsrcIdx > 0);
    rVl = xc->getRegOperand(this, vlsrcIdx);

    uint32_t microvlmax = VLEN / sew * (vflmul > 0 ? 1 : vflmul);
    uint32_t elem_num_per_vreg = microvlmax;

    %(vm_decl_rd)s;
    vwu vtmp, vs1;
    vu *Vs2=nullptr;

    auto func = %(code)s;
    auto reduce_loop =
        [&, this](bool middle, vwu vtmp) {
            vwu microop_result = middle ? vtmp : Vs2[0];
            for (uint32_t i = 0; i < microvlmax; i++) {
                uint32_t ei = i + microvlmax * this->microIdx;
                if ((ei < rVl) && (this->vm || elem_mask(v0, ei))) {
                    microop_result = func(microop_result, Vs2[i]).v;
                } else if (ei >= rVl) {
                    break;
                }
            }
            return microop_result;
        };

    vwu result;
    if (pos == FirstUop) {
        RiscvISAInst::VecRegContainer tmp_s0;
        xc->getRegOperand(this, 0,
            &tmp_s0);
        Vs2 = tmp_s0.as<vu>();
        result = reduce_loop(false, 0);
    } else if (pos == MiddleUop) {
        RiscvISAInst::VecRegContainer tmp_s0;
        xc->getRegOperand(this, 0,
            &tmp_s0);
        vtmp = tmp_s0.as<vwu>()[0];

        xc->getRegOperand(this, 1,
            &tmp_s0);
        Vs2 = tmp_s0.as<vu>();
        result = reduce_loop(true, vtmp);
    } else {
        COPY_OLD_VD();
        RiscvISAInst::VecRegContainer tmp_s0;
        xc->getRegOperand(this, 1,
            &tmp_s0);
        vtmp = tmp_s0.as<vwu>()[0];
        xc->getRegOperand(this, 2,
            &tmp_s0);
        vs1 = tmp_s0.as<vwu>()[0];
        result = func(vtmp, vs1).v;
    }
    Vd[0] = result;
    RegVal FFLAGS = xc->readMiscReg(MISCREG_FFLAGS);
    std::feclearexcept(FE_ALL_EXCEPT);
    FFLAGS |= softfloat_exceptionFlags;
    if (softfloat_exceptionFlags) {
        softfloat_exceptionFlags = 0;
        xc->setMiscReg(MISCREG_FFLAGS, FFLAGS);
    }

    xc->setRegOperand(this,0, &tmp_d0);
    if (traceData) {
        traceData->setData(tmp_d0);
    }

    return NoFault;
}

}};

def template VectorGatherMacroDeclare {{

template<typename ElemType, typename IndexType>
class %(class_name)s : public %(base_class)s{
private:
    %(reg_idx_arr_decl)s;
public:
    %(class_name)s(ExtMachInst _machInst);
    using %(base_class)s::generateDisassembly;
};

}};

def template VectorGatherMacroConstructor {{

template<typename ElemType, typename IndexType>
%(class_name)s<ElemType, IndexType>::%(class_name)s(ExtMachInst _machInst)
    : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s)
{
    const uint32_t vd_eewb = sizeof(ElemType);
    const uint32_t vs2_eewb = sizeof(ElemType);
    const uint32_t vs1_eewb = sizeof(IndexType);
    const bool vs1_split = vd_eewb > vs1_eewb;
    const int8_t vs1_emul = vlmul +
        (vs1_split ? -(vs2_eewb / vs1_eewb) : vs1_eewb / vs2_eewb);
    const uint8_t vs2_vregs = vlmul < 0 ? 1 : 1 << vlmul;
    const uint8_t vs1_vregs = vs1_emul < 0 ? 1 : 1 << vs1_emul;
    const uint8_t vd_vregs = vs2_vregs;

    StaticInstPtr microop;

    for (uint8_t i = 0; i < std::max(vs1_vregs, vd_vregs); i++) {
        for (uint8_t j = 0; j < vs2_vregs; j++) {
            microop = new %(class_name)sMicro<ElemType, IndexType>(_machInst, i * vs2_vregs + j);
            microop->setDelayedCommit();
            this->microops.push_back(microop);
        }
    }

    this->microops.front()->setFirstMicroop();
    this->microops.back()->setLastMicroop();
}

}};

def template VectorGatherMicroDeclare {{

template<typename ElemType, typename IndexType>
class %(class_name)s : public %(base_class)s
{
private:
    // vs2, vs1, vd, vm
    RegId srcRegIdxArr[8];
    RegId destRegIdxArr[1];
public:
    %(class_name)s(ExtMachInst _machInst,
                   uint8_t _microIdx);
    Fault execute(ExecContext* xc, Trace::InstRecord* traceData)const override;
    using %(base_class)s::generateDisassembly;
};

}};

def template VectorGatherMicroConstructor {{

template<typename ElemType, typename IndexType>
%(class_name)s<ElemType, IndexType>::%(class_name)s(ExtMachInst _machInst,
                                                    uint8_t _microIdx)
: %(base_class)s("%(mnemonic)s", _machInst,
                 %(op_class)s, _microIdx)
{
    uint32_t elem_num_per_vreg = VLEN / sew;
    vmi.rs = _microIdx * elem_num_per_vreg;
    vmi.re = (_microIdx+1) * elem_num_per_vreg;

    %(set_reg_idx_arr)s;
    _numSrcRegs = 0;
    _numDestRegs = 0;
    [[maybe_unused]] const uint32_t vd_eewb = sizeof(ElemType);
    [[maybe_unused]] const uint32_t vs2_eewb = sizeof(ElemType);
    [[maybe_unused]] const uint32_t vs1_eewb = sizeof(IndexType);
    const uint8_t vs1_split_num = (vd_eewb + vs1_eewb - 1) / vs1_eewb;
    const uint8_t vd_split_num = (vs1_eewb + vd_eewb - 1) / vd_eewb;
    const uint8_t vs2_vregs = vlmul < 0 ? 1 : 1 << vlmul;
    [[maybe_unused]] const uint8_t vs2_idx = _microIdx % vs2_vregs;
    [[maybe_unused]] const uint8_t vs1_idx =
        _microIdx / vs2_vregs / vs1_split_num;
    [[maybe_unused]] const uint8_t vd_idx =
        _microIdx / vs2_vregs / vd_split_num;
    %(set_dest_reg_idx)s;
    %(set_src_reg_idx)s;
}

}};

def template VectorGatherMicroExecute {{

template <typename ElemType, typename IndexType>
Fault
%(class_name)s<ElemType, IndexType>::execute(ExecContext* xc,
                                  Trace::InstRecord* traceData) const
{
    using vu [[maybe_unused]] = std::make_unsigned_t<ElemType>;

    if (machInst.vill) {
        return std::make_shared<IllegalInstFault>("VILL is set", machInst);
    }

    %(op_decl)s;
    %(op_rd)s;
    %(vm_decl_rd)s;
    COPY_OLD_VD();
    const uint32_t vlmax = VLEN / sew * vflmul;
    const uint8_t vd_eewb = sizeof(ElemType);
    const uint8_t vs1_eewb = sizeof(IndexType);
    const uint8_t vs2_eewb = sizeof(ElemType);
    const uint8_t vs1_split_num = (vd_eewb + vs1_eewb - 1) / vs1_eewb;
    const uint8_t vd_split_num = (vs1_eewb + vd_eewb - 1) / vd_eewb;
    [[maybe_unused]] const uint16_t vd_elems = VLENB / vd_eewb;
    [[maybe_unused]] const uint16_t vs1_elems = VLENB / vs1_eewb;
    [[maybe_unused]] const uint16_t vs2_elems = VLENB / vs2_eewb;
    [[maybe_unused]] const uint8_t vs2_vregs = vlmul < 0 ? 1 : 1 << vlmul;
    [[maybe_unused]] const uint8_t vs2_idx = microIdx % vs2_vregs;
    [[maybe_unused]] const uint8_t vs1_idx =
        microIdx / vs2_vregs / vs1_split_num;
    [[maybe_unused]] const uint8_t vd_idx =
        microIdx / vs2_vregs / vd_split_num;
    [[maybe_unused]] const uint16_t vs1_bias =
        vs1_elems * (vd_idx % vs1_split_num) / vs1_split_num;
    [[maybe_unused]] const uint16_t vd_bias =
        vd_elems * (vs1_idx % vd_split_num) / vd_split_num;
    uint32_t elem_num_per_vreg = VLENB / std::max(vd_eewb, vs1_eewb);

    %(code)s;
    %(op_wb)s;

    return NoFault;
}

}};

def template VectorGatherDecodeBlock {{

switch(machInst.vtype8.vsew) {
    case 0b000: {
        using elem_type [[maybe_unused]] = uint8_t;
        return new %(class_name)s<uint8_t, %(idx_type)s>(machInst);
    }
    case 0b001: {
        using elem_type [[maybe_unused]] = uint16_t;
        return new %(class_name)s<uint16_t, %(idx_type)s>(machInst);
    }
    case 0b010: {
        using elem_type [[maybe_unused]] = uint32_t;
        return new %(class_name)s<uint32_t, %(idx_type)s>(machInst);
    }
    case 0b011: {
        using elem_type [[maybe_unused]] = uint64_t;
        return new %(class_name)s<uint64_t, %(idx_type)s>(machInst);
    }
    default: GEM5_UNREACHABLE;
}

}};

def template VectorIntVxsatMacroDeclare {{

template<typename ElemType>
class %(class_name)s : public %(base_class)s{
private:
    %(reg_idx_arr_decl)s;
    bool vxsat = false;
public:
    %(class_name)s(ExtMachInst _machInst);
    using %(base_class)s::generateDisassembly;
};

}};

def template VectorIntVxsatMacroConstructor {{

template<typename ElemType>
%(class_name)s<ElemType>::%(class_name)s(ExtMachInst _machInst)
    : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s)
{
    const uint32_t num_microops = vflmul < 1 ? 1 : vflmul;

    StaticInstPtr microop;

    for (int i = 0; i < num_microops; ++i) {
        microop = new %(class_name)sMicro<ElemType>(_machInst, i, &vxsat);
        microop->setDelayedCommit();
        this->microops.push_back(microop);
    }
    microop = new VxsatMicroInst(&vxsat, _machInst);
    microop->setFlag(StaticInst::IsSerializeAfter);
    microop->setFlag(StaticInst::IsNonSpeculative);
    this->microops.push_back(microop);

    this->microops.front()->setFirstMicroop();
    this->microops.back()->setLastMicroop();
}
}};

def template VectorIntVxsatMicroDeclare {{

template<typename ElemType>
class %(class_name)s : public %(base_class)s
{
private:
    RegId srcRegIdxArr[8];
    RegId destRegIdxArr[1];
    bool* vxsatptr;
public:
    %(class_name)s(ExtMachInst _machInst,
                   uint8_t _microIdx, bool* vxsatptr);
    Fault execute(ExecContext* xc, Trace::InstRecord* traceData)const override;
    using %(base_class)s::generateDisassembly;
};

}};

def template VectorIntVxsatMicroConstructor {{

template<typename ElemType>
%(class_name)s<ElemType>::%(class_name)s(ExtMachInst _machInst,
    uint8_t _microIdx, bool* vxsatptr)
    : %(base_class)s("%(mnemonic)s", _machInst,
                     %(op_class)s, _microIdx)
{
    uint32_t elem_num_per_vreg = VLEN / sew;
    vmi.rs = _microIdx * elem_num_per_vreg;
    vmi.re = (_microIdx+1) * elem_num_per_vreg;
    this->vxsatptr = vxsatptr;
    %(set_reg_idx_arr)s;
    _numSrcRegs = 0;
    _numDestRegs = 0;
    %(set_dest_reg_idx)s;
    %(set_src_reg_idx)s;
}

}};

def template VectorSlideMacroDeclare {{

template<typename ElemType>
class %(class_name)s : public %(base_class)s {
private:
    %(reg_idx_arr_decl)s;
public:
    %(class_name)s(ExtMachInst _machInst);
    using %(base_class)s::generateDisassembly;
};

}};

def template VectorSlideUpMacroConstructor {{

template<typename ElemType>
%(class_name)s<ElemType>::%(class_name)s(ExtMachInst _machInst)
    : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s)
{
    const uint32_t elem_num_per_vreg = VLEN / sew;
    const uint32_t num_microops = vflmul < 1 ? 1 : vflmul;
    StaticInstPtr microop;

    int micro_idx = 0;
    VectorMicroInfo vmi;
    uint32_t off = machInst.vecimm;
    bool slide1up = false;
    if (strcmp("%(mnemonic)s", "vslide1up_vx") == 0
        || strcmp("%(mnemonic)s", "vfslide1up_vf") == 0) {
        off = 1;
        slide1up = true;
    }

    if (strcmp("%(mnemonic)s", "vslideup_vi") == 0 || slide1up) {
        // if i + off < elem_num_per_vreg: vd[i] = vs2[i + off]
        // if i < elem_num_per_vreg && i + off > elem_num_per_vreg: vd[i] = (vs2 + 1)[off - (i % elem_num_per_vreg)]
        for (int i = 0; i < num_microops; ++i) {
            int new_vs2 = VS2 + i - ((off + elem_num_per_vreg)/elem_num_per_vreg);
            if (new_vs2 < 0) {
                new_vs2 = 0;
            }
            vmi.rs = i * elem_num_per_vreg;
            vmi.re = (i+1) * elem_num_per_vreg;
            vmi.microVd = VD + i;
            vmi.microVs2 = new_vs2;
            if (vmi.microVd >= 32 || vmi.microVs2 >= 32) {
                break;
            }
            microop = new %(class_name)sMicro<ElemType>(_machInst, micro_idx++, vmi);
            microop->setDelayedCommit();
            this->microops.push_back(microop);
        }
    } else {
        for (int i = 0; i < num_microops; ++i) {
            for (int j = 0; j < num_microops; ++j) {
                vmi.rs = i * elem_num_per_vreg;
                vmi.re = (i+1) * elem_num_per_vreg;
                vmi.microVd = VD + i;
                vmi.microVs2 = VS2 + i - j - 1;
                if (vmi.microVd >= 32) {
                    break;
                }
                if (vmi.microVs2 < 0) {
                    vmi.microVs2 = 0;
                }
                microop = new %(class_name)sMicro<ElemType>(_machInst, micro_idx++, vmi);
                microop->setDelayedCommit();
                this->microops.push_back(microop);
            }
        }
    }

    this->microops.front()->setFirstMicroop();
    this->microops.back()->setLastMicroop();
}

}};

def template VectorSlideDownMacroConstructor {{

template<typename ElemType>
%(class_name)s<ElemType>::%(class_name)s(ExtMachInst _machInst)
    : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s)
{
    const uint32_t elem_num_per_vreg = VLEN / sew;
    const uint32_t num_microops = vflmul < 1 ? 1 : vflmul;
    StaticInstPtr microop;

    int micro_idx = 0;
    VectorMicroInfo vmi;
    uint32_t off = machInst.vecimm;
    bool slide1down = false;
    if (strcmp("%(mnemonic)s", "vslide1down_vx")==0
        || strcmp("%(mnemonic)s", "vfslide1down_vf")==0) {
        off = 1;
        slide1down = true;
    }

    if (strcmp("%(mnemonic)s", "vslidedown_vi")==0 || slide1down) {
        // if i + off < elem_num_per_vreg: vd[i] = vs2[i + off]
        // if i < elem_num_per_vreg && i + off > elem_num_per_vreg: vd[i] = (vs2 + 1)[off - (i % elem_num_per_vreg)]
        for (int i = 0; i < num_microops; ++i) {
            vmi.rs = i * elem_num_per_vreg;
            vmi.re = (i+1) * elem_num_per_vreg;
            vmi.microVd = VD + i;
            vmi.microVs2 = elem_gen_idx(VS2 + i, off, sew/8);
            if (vmi.microVd >= 32) {
                break;
            }
            if (vmi.microVs2 >= 32) {
                vmi.microVs2 = 31;
            }
            microop = new %(class_name)sMicro<ElemType>(_machInst, micro_idx++, vmi);
            microop->setDelayedCommit();
            this->microops.push_back(microop);
        }
    } else {
        for (int i = 0; i < num_microops; ++i) {
            for (int j = i; j < RiscvISA::VLEN/8; ++j) {
                vmi.rs = i * elem_num_per_vreg;
                vmi.re = (i+1) * elem_num_per_vreg;
                vmi.microVd = VD + i;
                vmi.microVs2 = VS2 + j;
                if (vmi.microVd >= 32) {
                    break;
                }
                if (vmi.microVs2 >= 32) {
                    vmi.microVs2 = 31;
                }
                microop = new %(class_name)sMicro<ElemType>(_machInst, micro_idx++, vmi);
                microop->setDelayedCommit();
                this->microops.push_back(microop);
            }
        }
    }

    this->microops.front()->setFirstMicroop();
    this->microops.back()->setLastMicroop();
}

}};

def template VectorSlideMicroDeclare {{

template<typename ElemType>
class %(class_name)s : public %(base_class)s
{
private:
    // vs2, vs1, vs3(old_vd), vm for *.vv, *.vx
    // vs2, (old_vd), vm for *.vi
    RegId srcRegIdxArr[8];
    RegId destRegIdxArr[1];
public:
    %(class_name)s(ExtMachInst _machInst, uint8_t _microIdx, VectorMicroInfo& _vmi);
    Fault execute(ExecContext* xc, Trace::InstRecord* traceData)const override;
    using %(base_class)s::generateDisassembly;
};

}};

def template VectorSlideMicroConstructor {{

template<typename ElemType>
%(class_name)s<ElemType>::%(class_name)s(ExtMachInst _machInst, uint8_t _microIdx, VectorMicroInfo& _vmi)
    : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s, _microIdx)
{
    %(set_reg_idx_arr)s;
    vmi = _vmi;
    assert(~vmi.microVd);
    assert(~vmi.microVs2);
    _numSrcRegs = 0;
    _numDestRegs = 0;
    %(set_dest_reg_idx)s;
    %(set_src_reg_idx)s;
}

}};

def template VectorSlideMicroExecute {{

template<typename ElemType>
Fault
%(class_name)s<ElemType>::execute(ExecContext* xc,
                                  Trace::InstRecord* traceData) const
{
    using vu [[maybe_unused]] = std::make_unsigned_t<ElemType>;
    using vi [[maybe_unused]] = std::make_signed_t<ElemType>;

    if (machInst.vill) {
        return std::make_shared<IllegalInstFault>("VILL is set", machInst);
    }
    [[maybe_unused]]const uint32_t vlmax = VLEN / sew * vflmul;
    const int elem_num_per_vreg = VLEN / sew;

    %(op_decl)s;
    %(op_rd)s;
    %(vm_decl_rd)s;
    COPY_OLD_VD();
    %(code)s;
    %(op_wb)s;

    return NoFault;
};

}};

def template VectorFloatSlideMicroExecute {{

template<typename ElemType>
Fault
%(class_name)s<ElemType>::execute(ExecContext* xc,
                                  Trace::InstRecord* traceData) const
{
    using et = ElemType;
    using vu = decltype(et::v);

    if (machInst.vill) {
        return std::make_shared<IllegalInstFault>("VILL is set", machInst);
    }
    [[maybe_unused]]const uint32_t vlmax = VLEN / sew * vflmul;
    const uint32_t elem_num_per_vreg = VLEN / sew;

    %(op_decl)s;
    %(op_rd)s;
    %(vm_decl_rd)s;
    COPY_OLD_VD();
    %(code)s;
    %(op_wb)s;

    return NoFault;
};

}};
